{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zu_I3Lri8PsT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from tensorflow.keras import layers, activations, models, preprocessing\n",
        "from tensorflow.keras import preprocessing, utils\n",
        "import os\n",
        "import yaml\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bVEHT1fDBkg",
        "outputId": "db4c1ed6-eb60-4a67-b297-12bab55476cd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dir_path = '/content/drive/MyDrive/My_Work/First_Chat_Bot/chatt'\n",
        "files_list = os.listdir(dir_path + os.sep)"
      ],
      "metadata": {
        "id": "AwkcRCK4D4i7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Tokenize:\n",
        "    \"\"\"\n",
        "    A class used to tokenize conversational data from YAML files.\n",
        "    It separates questions and answers, removes invalid statements,\n",
        "    and generates a tokenizer for vocabulary extraction.\n",
        "\n",
        "    Attributes:\n",
        "        files_list (list): List of file names containing conversational data.\n",
        "        dir_path (str): Directory path where the files are stored.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, files_list, dir_path):\n",
        "        \"\"\"\n",
        "        Initializes the Tokenize class with a list of files and their directory path.\n",
        "\n",
        "        Args:\n",
        "            files_list (list): List of YAML file names containing conversations.\n",
        "            dir_path (str): Path to the directory containing the YAML files.\n",
        "        \"\"\"\n",
        "        self.files_list = files_list\n",
        "        self.dir_path = dir_path\n",
        "\n",
        "    def separate_question_answers(self):\n",
        "        \"\"\"\n",
        "        Extracts questions and answers from the conversational data in the provided files.\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple containing two lists - questions and their corresponding answers.\n",
        "        \"\"\"\n",
        "        questions = list()\n",
        "        answers = list()\n",
        "        for filepath in self.files_list:\n",
        "            stream = open(self.dir_path + os.sep + filepath, 'rb')\n",
        "            docs = yaml.safe_load(stream)\n",
        "            conversations = docs['conversations']\n",
        "            for con in conversations:\n",
        "                if len(con) > 2:\n",
        "                    questions.append(con[0])\n",
        "                    replies = con[1:]\n",
        "                    ans = \"\"\n",
        "                    for rep in replies:\n",
        "                        ans += \" \" + rep\n",
        "                    answers.append(ans)\n",
        "                elif len(con) > 1:\n",
        "                    questions.append(con[0])\n",
        "                    answers.append(con[1])\n",
        "\n",
        "        return questions, answers\n",
        "\n",
        "    def remove_not_str_statements(self):\n",
        "        \"\"\"\n",
        "        Cleans the extracted data by removing answers that are not strings\n",
        "        and appends start and end tags to valid answers.\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple containing two lists - cleaned questions and tagged answers.\n",
        "        \"\"\"\n",
        "        questions, answers = self.separate_question_answers()\n",
        "        answers_with_tags = list()\n",
        "        for i in range(len(answers)):\n",
        "            if type(answers[i]) == str:\n",
        "                answers_with_tags.append(answers[i])\n",
        "            else:\n",
        "                questions.pop(i)\n",
        "        answers = list()\n",
        "        for i in range(len(answers_with_tags)):\n",
        "            answers.append(\" <start> \" + answers_with_tags[i] + \" <end> \")\n",
        "\n",
        "        return questions, answers\n",
        "\n",
        "    def tokenization(self):\n",
        "        \"\"\"\n",
        "        Tokenizes the questions and answers to create a vocabulary.\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple containing the tokenizer object and the vocabulary size.\n",
        "        \"\"\"\n",
        "        questions, answers = self.remove_not_str_statements()\n",
        "        tokenizer = preprocessing.text.Tokenizer()\n",
        "        tokenizer.fit_on_texts(questions + answers)\n",
        "        vocab_size = len(tokenizer.word_index) + 1\n",
        "        # print(\"vocab size is:\", vocab_size)\n",
        "        return tokenizer, vocab_size\n"
      ],
      "metadata": {
        "id": "sF4w1hRgaaED"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer, vocab_size = Tokenize(files_list, dir_path).tokenization()\n",
        "questions, answers = Tokenize(files_list, dir_path).remove_not_str_statements()\n"
      ],
      "metadata": {
        "id": "wxlGetgddb3s"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "JIPmmREaSQvP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from gensim.models import Word2Vec\n",
        "import re"
      ],
      "metadata": {
        "id": "nsS9LZvpfXU9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataprepration:\n",
        "    \"\"\"\n",
        "    A class for preparing encoder and decoder input/output data\n",
        "    for training a sequence-to-sequence model.\n",
        "\n",
        "    Attributes:\n",
        "        questions (list): List of question sentences.\n",
        "        answers (list): List of answer sentences.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, questions, answers):\n",
        "        \"\"\"\n",
        "        Initializes the Dataprepration class with questions and answers.\n",
        "\n",
        "        Args:\n",
        "            questions (list): List of question sentences.\n",
        "            answers (list): List of answer sentences.\n",
        "        \"\"\"\n",
        "        self.questions = questions\n",
        "        self.answers = answers\n",
        "\n",
        "    def encoder_input_data(self):\n",
        "        \"\"\"\n",
        "        Prepares the encoder input data by tokenizing and padding the questions.\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple containing:\n",
        "                - encoder_input_d (numpy array): Padded, tokenized questions.\n",
        "                - maxlen_questions (int): Maximum sequence length of questions.\n",
        "                - vocab_size (int): Vocabulary size of the tokenizer.\n",
        "        \"\"\"\n",
        "        tokenizer, vocab_size = Tokenize(files_list, dir_path).tokenization()\n",
        "        tokenized_questions = tokenizer.texts_to_sequences(self.questions)\n",
        "        maxlen_questions = max([len(x) for x in tokenized_questions])\n",
        "        padded_questions = preprocessing.sequence.pad_sequences(tokenized_questions, maxlen=maxlen_questions, padding='post')\n",
        "        encoder_input_d = np.array(padded_questions)\n",
        "        return encoder_input_d, maxlen_questions, vocab_size\n",
        "\n",
        "    def decoder_input_data(self):\n",
        "        \"\"\"\n",
        "        Prepares the decoder input data by tokenizing and padding the answers.\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple containing:\n",
        "                - decoder_input_d (numpy array): Padded, tokenized answers.\n",
        "                - maxlen_answers (int): Maximum sequence length of answers.\n",
        "        \"\"\"\n",
        "        tokenizer, vocab_size = Tokenize(files_list, dir_path).tokenization()\n",
        "        tokenized_answers = tokenizer.texts_to_sequences(self.answers)\n",
        "        maxlen_answers = max([len(x) for x in tokenized_answers])\n",
        "        padded_answers = preprocessing.sequence.pad_sequences(tokenized_answers, maxlen=maxlen_answers, padding='post')\n",
        "        decoder_input_d = np.array(padded_answers)\n",
        "        return decoder_input_d, maxlen_answers\n",
        "\n",
        "    def decoder_output_data(self):\n",
        "        \"\"\"\n",
        "        Prepares the decoder output data by shifting tokenized answers,\n",
        "        padding them, and converting them to one-hot encoded format.\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple containing:\n",
        "                - decoder_output_d (numpy array): One-hot encoded, padded tokenized answers.\n",
        "                - maxlen_answers (int): Maximum sequence length of answers.\n",
        "        \"\"\"\n",
        "        tokenized_answers = tokenizer.texts_to_sequences(self.answers)\n",
        "        maxlen_answers = max([len(x) for x in tokenized_answers])\n",
        "        for i in range(len(tokenized_answers)):\n",
        "            tokenized_answers[i] = tokenized_answers[i][1:]  # Shift the answers by one time step\n",
        "        padded_answers = preprocessing.sequence.pad_sequences(tokenized_answers, maxlen=maxlen_answers, padding='post')\n",
        "        onehot_answers = utils.to_categorical(padded_answers, vocab_size)\n",
        "        decoder_output_d = np.array(onehot_answers)\n",
        "        return decoder_output_d, maxlen_answers\n"
      ],
      "metadata": {
        "id": "XcLFt9KGWY1C"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder_Decoder:\n",
        "    \"\"\"\n",
        "    A class to create and train an encoder-decoder model using an LSTM-based sequence-to-sequence architecture.\n",
        "\n",
        "    Attributes:\n",
        "        questions (list): List of input questions.\n",
        "        answers (list): List of corresponding answers.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, questions, answers):\n",
        "        \"\"\"\n",
        "        Initializes the Encoder_Decoder class with input questions and answers.\n",
        "\n",
        "        Args:\n",
        "            questions (list): List of input question sentences.\n",
        "            answers (list): List of target answer sentences.\n",
        "        \"\"\"\n",
        "        self.questions = questions\n",
        "        self.answers = answers\n",
        "\n",
        "    def modeling(self):\n",
        "        \"\"\"\n",
        "        Creates and trains the encoder-decoder model. This includes:\n",
        "        - Preparing encoder input data, decoder input data, and decoder output data.\n",
        "        - Defining the model architecture, including embedding layers, LSTM layers, and dense layers.\n",
        "        - Compiling and training the model on the prepared data.\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple containing:\n",
        "                - model (tf.keras.Model): The trained encoder-decoder model.\n",
        "                - decoder_embedding (tf.Tensor): The embedding layer for the decoder inputs.\n",
        "                - decoder_dense (tf.Tensor): The dense layer for generating predictions.\n",
        "                - decoder_inputs (tf.Tensor): Input tensor for the decoder.\n",
        "                - decoder_lstm (tf.keras.layers.LSTM): The LSTM layer used in the decoder.\n",
        "                - encoder_inputs (tf.Tensor): Input tensor for the encoder.\n",
        "                - encoder_states (list): List of encoder states (state_h and state_c).\n",
        "        \"\"\"\n",
        "        preparation_object = Dataprepration(self.questions, self.answers)\n",
        "\n",
        "        # Prepare input and output data\n",
        "        encoder_input_da, maxlen_questions, VOCAB_SIZE = preparation_object.encoder_input_data()\n",
        "        decoder_input_da, maxlen_answers = preparation_object.decoder_input_data()\n",
        "        decoder_output_da, _ = preparation_object.decoder_output_data()\n",
        "\n",
        "        # Define encoder\n",
        "        encoder_inputs = tf.keras.layers.Input(shape=(maxlen_questions,))\n",
        "        encoder_embedding = tf.keras.layers.Embedding(VOCAB_SIZE, 200, mask_zero=True)(encoder_inputs)\n",
        "        encoder_outputs, state_h, state_c = tf.keras.layers.LSTM(200, return_state=True)(encoder_embedding)\n",
        "        encoder_states = [state_h, state_c]\n",
        "\n",
        "        # Define decoder\n",
        "        decoder_inputs = tf.keras.layers.Input(shape=(maxlen_answers,))\n",
        "        decoder_embedding = tf.keras.layers.Embedding(VOCAB_SIZE, 200, mask_zero=True)(decoder_inputs)\n",
        "        decoder_lstm = tf.keras.layers.LSTM(200, return_state=True, return_sequences=True)\n",
        "        decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
        "        decoder_dense = tf.keras.layers.Dense(VOCAB_SIZE, activation=tf.keras.activations.softmax)\n",
        "        output = decoder_dense(decoder_outputs)\n",
        "\n",
        "        # Create the model\n",
        "        model = tf.keras.models.Model([encoder_inputs, decoder_inputs], output)\n",
        "        model.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='categorical_crossentropy')\n",
        "        model.summary()\n",
        "\n",
        "        # Train the model\n",
        "        model.fit([encoder_input_da, decoder_input_da], decoder_output_da, batch_size=50, epochs=100, verbose=0)\n",
        "\n",
        "        return model, decoder_embedding, decoder_dense, decoder_inputs, decoder_lstm, encoder_inputs, encoder_states\n"
      ],
      "metadata": {
        "id": "dpmkp3d2o3ck"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, decoder_embedding, decoder_dense, decoder_inputs,decoder_lstm , encoder_inputs, encoder_states = Encoder_Decoder(questions, answers).modeling()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "pbLgkXZHo5qb",
        "outputId": "dc013f1e-38f5-44fb-bf42-d6a4f7af60a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m22\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ input_layer_1             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m74\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m22\u001b[0m, \u001b[38;5;34m200\u001b[0m)        │        \u001b[38;5;34m378,800\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ not_equal (\u001b[38;5;33mNotEqual\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m22\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m74\u001b[0m, \u001b[38;5;34m200\u001b[0m)        │        \u001b[38;5;34m378,800\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)               │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,   │        \u001b[38;5;34m320,800\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n",
              "│                           │ \u001b[38;5;34m200\u001b[0m), (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m)]     │                │ not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)             │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m74\u001b[0m, \u001b[38;5;34m200\u001b[0m),      │        \u001b[38;5;34m320,800\u001b[0m │ embedding_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
              "│                           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,    │                │ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m], lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m2\u001b[0m] │\n",
              "│                           │ \u001b[38;5;34m200\u001b[0m)]                  │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m74\u001b[0m, \u001b[38;5;34m1894\u001b[0m)       │        \u001b[38;5;34m380,694\u001b[0m │ lstm_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ input_layer_1             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">74</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">378,800</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ not_equal (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">74</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">378,800</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)               │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">320,800</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n",
              "│                           │ <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)]     │                │ not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)             │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">74</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>),      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">320,800</span> │ embedding_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
              "│                           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,    │                │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>], lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>] │\n",
              "│                           │ <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)]                  │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">74</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1894</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">380,694</span> │ lstm_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,779,894\u001b[0m (6.79 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,779,894</span> (6.79 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,779,894\u001b[0m (6.79 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,779,894</span> (6.79 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_inference_models():\n",
        "    \"\"\"\n",
        "    Creates the inference models for both the encoder and decoder parts of a sequence-to-sequence model.\n",
        "\n",
        "    The encoder model is used to encode input sequences and return the internal states,\n",
        "    while the decoder model is used for making predictions one timestep at a time\n",
        "    during the inference phase.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing:\n",
        "            - encoder_model (tf.keras.Model): The model for encoding input sequences.\n",
        "            - decoder_model (tf.keras.Model): The model for decoding and predicting output sequences.\n",
        "    \"\"\"\n",
        "    # Define the encoder inference model\n",
        "    encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n",
        "\n",
        "    # Define placeholders for decoder states during inference\n",
        "    decoder_state_input_h = tf.keras.layers.Input(shape=(200,))\n",
        "    decoder_state_input_c = tf.keras.layers.Input(shape=(200,))\n",
        "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "    # Use the decoder LSTM with the states as initial inputs\n",
        "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
        "        decoder_embedding, initial_state=decoder_states_inputs\n",
        "    )\n",
        "    decoder_states = [state_h, state_c]\n",
        "\n",
        "    # Apply the dense layer to obtain predictions\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "    # Define the decoder inference model\n",
        "    decoder_model = tf.keras.models.Model(\n",
        "        [decoder_inputs] + decoder_states_inputs,\n",
        "        [decoder_outputs] + decoder_states\n",
        "    )\n",
        "\n",
        "    return encoder_model, decoder_model\n"
      ],
      "metadata": {
        "id": "pn5w-pXuo5oI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def str_to_tokens(sentence: str):\n",
        "    \"\"\"\n",
        "    Converts a given sentence into tokenized and padded sequences for use as input to the encoder model.\n",
        "\n",
        "    Args:\n",
        "        sentence (str): The input sentence to be tokenized.\n",
        "\n",
        "    Returns:\n",
        "        numpy array: A padded sequence of tokens corresponding to the input sentence.\n",
        "    \"\"\"\n",
        "    # Extract the maximum question length from the encoder input data\n",
        "    _, maxlen_questions, _ = Dataprepration(questions, answers).encoder_input_data()\n",
        "\n",
        "    # Convert the sentence to lowercase and split it into words\n",
        "    words = sentence.lower().split()\n",
        "    tokens_list = list()\n",
        "\n",
        "    # Convert each word into its corresponding token from the tokenizer's word index\n",
        "    for word in words:\n",
        "        tokens_list.append(tokenizer.word_index[word])\n",
        "\n",
        "    # Pad the tokenized sequence to match the maximum question length\n",
        "    return preprocessing.sequence.pad_sequences([tokens_list], maxlen=maxlen_questions, padding='post')\n"
      ],
      "metadata": {
        "id": "LZ0CGlUHo5la"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chatbot_inference(tokenizer, num_responses=10, max_translation_length=72):\n",
        "    \"\"\"\n",
        "    Handles the chatbot inference process, allowing user interaction with the trained encoder-decoder model.\n",
        "\n",
        "    The function takes user input, processes it through the encoder-decoder model, and generates a decoded response.\n",
        "\n",
        "    Args:\n",
        "        enc_model (tf.keras.Model): The trained encoder inference model.\n",
        "        dec_model (tf.keras.Model): The trained decoder inference model.\n",
        "        tokenizer (tf.keras.preprocessing.text.Tokenizer): The tokenizer used for encoding and decoding text.\n",
        "        num_responses (int, optional): Number of questions the chatbot will respond to. Default is 10.\n",
        "        max_translation_length (int, optional): Maximum length of the generated translation. Default is 72.\n",
        "\n",
        "    Returns:\n",
        "        None: The function prints decoded translations to the console.\n",
        "    \"\"\"\n",
        "    enc_model , dec_model = make_inference_models()\n",
        "    for _ in range(num_responses):\n",
        "        # Take user input\n",
        "        user_input = input('Enter question: ')\n",
        "\n",
        "        # Encode the input using the encoder model\n",
        "        states_values = enc_model.predict(str_to_tokens(user_input))\n",
        "\n",
        "        # Initialize the target sequence with the \"start\" token\n",
        "        empty_target_seq = np.zeros((1, 1))\n",
        "        empty_target_seq[0, 0] = tokenizer.word_index['start']\n",
        "\n",
        "        # Initialize variables for the decoding loop\n",
        "        stop_condition = False\n",
        "        decoded_translation = ''\n",
        "\n",
        "        while not stop_condition:\n",
        "            # Predict the next token and updated decoder states\n",
        "            dec_outputs, h, c = dec_model.predict([empty_target_seq] + states_values)\n",
        "\n",
        "            # Get the token index with the highest probability\n",
        "            sampled_word_index = np.argmax(dec_outputs[0, -1, :])\n",
        "            sampled_word = None\n",
        "\n",
        "            # Convert the token index back to the word\n",
        "            for word, index in tokenizer.word_index.items():\n",
        "                if sampled_word_index == index:\n",
        "                    decoded_translation += ' {}'.format(word)\n",
        "                    sampled_word = word\n",
        "                    break\n",
        "\n",
        "            # Check if the stop condition is met\n",
        "            if sampled_word == 'end' or len(decoded_translation.split()) > max_translation_length:\n",
        "                stop_condition = True\n",
        "\n",
        "            # Update the target sequence and states for the next timestep\n",
        "            empty_target_seq = np.zeros((1, 1))\n",
        "            empty_target_seq[0, 0] = sampled_word_index\n",
        "            states_values = [h, c]\n",
        "\n",
        "        # Print the generated response\n",
        "        print(decoded_translation)\n",
        "\n",
        "\n",
        "chatbot_inference(tokenizer, num_responses=10, max_translation_length=72)"
      ],
      "metadata": {
        "id": "6CnI5llN67lD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}